
# 合并 lora 权重时，请勿使用量化模型或 quantization_bit

# model
model_name_or_path: /home/wangguisen/models/Qwen1.5-14B-Chat
adapter_name_or_path: ./weights/checkpoint-1500
template: qwen
finetuning_type: lora

# export
export_dir: ./weights/qwen_lora_sft_1500
export_size: 2
export_device: cuda
export_legacy_format: false

# CUDA_VISIBLE_DEVICES=3 llamafactory-cli export yamls/merge_lora.yaml