{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 9.830508474576272,
  "eval_steps": 500,
  "global_step": 290,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.03389830508474576,
      "grad_norm": 1.86612069606781,
      "learning_rate": 3.448275862068966e-06,
      "loss": 1.9214,
      "step": 1
    },
    {
      "epoch": 0.06779661016949153,
      "grad_norm": 1.2739046812057495,
      "learning_rate": 6.896551724137932e-06,
      "loss": 1.7213,
      "step": 2
    },
    {
      "epoch": 0.1016949152542373,
      "grad_norm": 1.601440668106079,
      "learning_rate": 1.0344827586206897e-05,
      "loss": 1.5492,
      "step": 3
    },
    {
      "epoch": 0.13559322033898305,
      "grad_norm": 1.8314745426177979,
      "learning_rate": 1.3793103448275863e-05,
      "loss": 1.9825,
      "step": 4
    },
    {
      "epoch": 0.1694915254237288,
      "grad_norm": 1.870347499847412,
      "learning_rate": 1.7241379310344828e-05,
      "loss": 2.0117,
      "step": 5
    },
    {
      "epoch": 0.2033898305084746,
      "grad_norm": 2.5482547283172607,
      "learning_rate": 2.0689655172413793e-05,
      "loss": 1.9978,
      "step": 6
    },
    {
      "epoch": 0.23728813559322035,
      "grad_norm": 1.5674110651016235,
      "learning_rate": 2.413793103448276e-05,
      "loss": 1.8499,
      "step": 7
    },
    {
      "epoch": 0.2711864406779661,
      "grad_norm": 1.6390602588653564,
      "learning_rate": 2.7586206896551727e-05,
      "loss": 1.7686,
      "step": 8
    },
    {
      "epoch": 0.3050847457627119,
      "grad_norm": 1.3365757465362549,
      "learning_rate": 3.103448275862069e-05,
      "loss": 1.6434,
      "step": 9
    },
    {
      "epoch": 0.3389830508474576,
      "grad_norm": 1.6155784130096436,
      "learning_rate": 3.4482758620689657e-05,
      "loss": 1.5901,
      "step": 10
    },
    {
      "epoch": 0.3728813559322034,
      "grad_norm": 1.53032648563385,
      "learning_rate": 3.793103448275862e-05,
      "loss": 1.7174,
      "step": 11
    },
    {
      "epoch": 0.4067796610169492,
      "grad_norm": 2.1432816982269287,
      "learning_rate": 4.1379310344827587e-05,
      "loss": 1.9392,
      "step": 12
    },
    {
      "epoch": 0.4406779661016949,
      "grad_norm": 1.2635302543640137,
      "learning_rate": 4.482758620689655e-05,
      "loss": 1.4583,
      "step": 13
    },
    {
      "epoch": 0.4745762711864407,
      "grad_norm": 1.5402227640151978,
      "learning_rate": 4.827586206896552e-05,
      "loss": 1.7792,
      "step": 14
    },
    {
      "epoch": 0.5084745762711864,
      "grad_norm": 1.358839511871338,
      "learning_rate": 5.172413793103449e-05,
      "loss": 1.7123,
      "step": 15
    },
    {
      "epoch": 0.5423728813559322,
      "grad_norm": 1.4042251110076904,
      "learning_rate": 5.517241379310345e-05,
      "loss": 1.7861,
      "step": 16
    },
    {
      "epoch": 0.576271186440678,
      "grad_norm": 1.3772634267807007,
      "learning_rate": 5.862068965517241e-05,
      "loss": 1.6361,
      "step": 17
    },
    {
      "epoch": 0.6101694915254238,
      "grad_norm": 1.3229819536209106,
      "learning_rate": 6.206896551724138e-05,
      "loss": 1.6519,
      "step": 18
    },
    {
      "epoch": 0.6440677966101694,
      "grad_norm": 1.2396175861358643,
      "learning_rate": 6.551724137931034e-05,
      "loss": 1.6136,
      "step": 19
    },
    {
      "epoch": 0.6779661016949152,
      "grad_norm": 1.3823648691177368,
      "learning_rate": 6.896551724137931e-05,
      "loss": 1.5732,
      "step": 20
    },
    {
      "epoch": 0.711864406779661,
      "grad_norm": 1.2443825006484985,
      "learning_rate": 7.241379310344828e-05,
      "loss": 1.4389,
      "step": 21
    },
    {
      "epoch": 0.7457627118644068,
      "grad_norm": 1.1876063346862793,
      "learning_rate": 7.586206896551724e-05,
      "loss": 1.5803,
      "step": 22
    },
    {
      "epoch": 0.7796610169491526,
      "grad_norm": 1.0845916271209717,
      "learning_rate": 7.931034482758621e-05,
      "loss": 1.4602,
      "step": 23
    },
    {
      "epoch": 0.8135593220338984,
      "grad_norm": 0.9776142835617065,
      "learning_rate": 8.275862068965517e-05,
      "loss": 1.6296,
      "step": 24
    },
    {
      "epoch": 0.847457627118644,
      "grad_norm": 1.0920374393463135,
      "learning_rate": 8.620689655172413e-05,
      "loss": 1.3911,
      "step": 25
    },
    {
      "epoch": 0.8813559322033898,
      "grad_norm": 1.4346297979354858,
      "learning_rate": 8.96551724137931e-05,
      "loss": 1.9185,
      "step": 26
    },
    {
      "epoch": 0.9152542372881356,
      "grad_norm": 1.113480806350708,
      "learning_rate": 9.310344827586207e-05,
      "loss": 1.4148,
      "step": 27
    },
    {
      "epoch": 0.9491525423728814,
      "grad_norm": 1.6219762563705444,
      "learning_rate": 9.655172413793105e-05,
      "loss": 1.5586,
      "step": 28
    },
    {
      "epoch": 0.9830508474576272,
      "grad_norm": 1.082411289215088,
      "learning_rate": 0.0001,
      "loss": 1.2242,
      "step": 29
    },
    {
      "epoch": 1.0169491525423728,
      "grad_norm": 1.1471855640411377,
      "learning_rate": 9.999637795788383e-05,
      "loss": 1.2342,
      "step": 30
    },
    {
      "epoch": 1.0508474576271187,
      "grad_norm": 1.387888789176941,
      "learning_rate": 9.99855123563029e-05,
      "loss": 1.5678,
      "step": 31
    },
    {
      "epoch": 1.0847457627118644,
      "grad_norm": 1.1172118186950684,
      "learning_rate": 9.996740476948385e-05,
      "loss": 1.3145,
      "step": 32
    },
    {
      "epoch": 1.11864406779661,
      "grad_norm": 1.3409022092819214,
      "learning_rate": 9.994205782088438e-05,
      "loss": 1.4448,
      "step": 33
    },
    {
      "epoch": 1.152542372881356,
      "grad_norm": 1.3770214319229126,
      "learning_rate": 9.990947518281311e-05,
      "loss": 1.2242,
      "step": 34
    },
    {
      "epoch": 1.1864406779661016,
      "grad_norm": 1.425779104232788,
      "learning_rate": 9.98696615758975e-05,
      "loss": 1.3693,
      "step": 35
    },
    {
      "epoch": 1.2203389830508475,
      "grad_norm": 1.2375417947769165,
      "learning_rate": 9.982262276840002e-05,
      "loss": 1.3677,
      "step": 36
    },
    {
      "epoch": 1.2542372881355932,
      "grad_norm": 1.3123887777328491,
      "learning_rate": 9.976836557538234e-05,
      "loss": 1.1265,
      "step": 37
    },
    {
      "epoch": 1.288135593220339,
      "grad_norm": 1.694014549255371,
      "learning_rate": 9.970689785771798e-05,
      "loss": 1.2638,
      "step": 38
    },
    {
      "epoch": 1.3220338983050848,
      "grad_norm": 1.6273926496505737,
      "learning_rate": 9.963822852095345e-05,
      "loss": 1.4845,
      "step": 39
    },
    {
      "epoch": 1.3559322033898304,
      "grad_norm": 1.5396255254745483,
      "learning_rate": 9.956236751401791e-05,
      "loss": 1.4981,
      "step": 40
    },
    {
      "epoch": 1.3898305084745763,
      "grad_norm": 2.313821792602539,
      "learning_rate": 9.947932582778188e-05,
      "loss": 1.4611,
      "step": 41
    },
    {
      "epoch": 1.423728813559322,
      "grad_norm": 1.4896461963653564,
      "learning_rate": 9.938911549346472e-05,
      "loss": 1.2719,
      "step": 42
    },
    {
      "epoch": 1.457627118644068,
      "grad_norm": 1.2094799280166626,
      "learning_rate": 9.929174958089167e-05,
      "loss": 1.1459,
      "step": 43
    },
    {
      "epoch": 1.4915254237288136,
      "grad_norm": 1.384128451347351,
      "learning_rate": 9.918724219660013e-05,
      "loss": 1.1644,
      "step": 44
    },
    {
      "epoch": 1.5254237288135593,
      "grad_norm": 2.170381546020508,
      "learning_rate": 9.907560848179606e-05,
      "loss": 1.2208,
      "step": 45
    },
    {
      "epoch": 1.559322033898305,
      "grad_norm": 1.5047088861465454,
      "learning_rate": 9.895686461016007e-05,
      "loss": 1.0377,
      "step": 46
    },
    {
      "epoch": 1.5932203389830508,
      "grad_norm": 1.3559006452560425,
      "learning_rate": 9.883102778550434e-05,
      "loss": 1.1179,
      "step": 47
    },
    {
      "epoch": 1.6271186440677967,
      "grad_norm": 1.3929275274276733,
      "learning_rate": 9.869811623928001e-05,
      "loss": 1.0943,
      "step": 48
    },
    {
      "epoch": 1.6610169491525424,
      "grad_norm": 1.6793968677520752,
      "learning_rate": 9.855814922793582e-05,
      "loss": 1.0801,
      "step": 49
    },
    {
      "epoch": 1.694915254237288,
      "grad_norm": 1.303082823753357,
      "learning_rate": 9.841114703012817e-05,
      "loss": 1.2356,
      "step": 50
    },
    {
      "epoch": 1.7288135593220337,
      "grad_norm": 1.5111669301986694,
      "learning_rate": 9.825713094378311e-05,
      "loss": 1.1696,
      "step": 51
    },
    {
      "epoch": 1.7627118644067796,
      "grad_norm": 1.7959067821502686,
      "learning_rate": 9.80961232830107e-05,
      "loss": 0.9606,
      "step": 52
    },
    {
      "epoch": 1.7966101694915255,
      "grad_norm": 1.2382816076278687,
      "learning_rate": 9.792814737487207e-05,
      "loss": 1.1616,
      "step": 53
    },
    {
      "epoch": 1.8305084745762712,
      "grad_norm": 1.2038748264312744,
      "learning_rate": 9.775322755599978e-05,
      "loss": 1.1711,
      "step": 54
    },
    {
      "epoch": 1.8644067796610169,
      "grad_norm": 1.5257947444915771,
      "learning_rate": 9.757138916907185e-05,
      "loss": 1.2704,
      "step": 55
    },
    {
      "epoch": 1.8983050847457628,
      "grad_norm": 1.54351806640625,
      "learning_rate": 9.738265855914013e-05,
      "loss": 1.2922,
      "step": 56
    },
    {
      "epoch": 1.9322033898305084,
      "grad_norm": 1.2814191579818726,
      "learning_rate": 9.71870630698133e-05,
      "loss": 1.1354,
      "step": 57
    },
    {
      "epoch": 1.9661016949152543,
      "grad_norm": 1.1678907871246338,
      "learning_rate": 9.698463103929542e-05,
      "loss": 1.0016,
      "step": 58
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.623552918434143,
      "learning_rate": 9.677539179628005e-05,
      "loss": 1.3558,
      "step": 59
    },
    {
      "epoch": 2.0338983050847457,
      "grad_norm": 1.5033836364746094,
      "learning_rate": 9.655937565570123e-05,
      "loss": 1.1296,
      "step": 60
    },
    {
      "epoch": 2.0677966101694913,
      "grad_norm": 1.1048749685287476,
      "learning_rate": 9.633661391434131e-05,
      "loss": 1.1597,
      "step": 61
    },
    {
      "epoch": 2.1016949152542375,
      "grad_norm": 1.229082703590393,
      "learning_rate": 9.610713884629666e-05,
      "loss": 0.9794,
      "step": 62
    },
    {
      "epoch": 2.135593220338983,
      "grad_norm": 1.290874719619751,
      "learning_rate": 9.587098369830172e-05,
      "loss": 1.0956,
      "step": 63
    },
    {
      "epoch": 2.169491525423729,
      "grad_norm": 1.554504156112671,
      "learning_rate": 9.562818268491216e-05,
      "loss": 1.3456,
      "step": 64
    },
    {
      "epoch": 2.2033898305084745,
      "grad_norm": 1.649438500404358,
      "learning_rate": 9.537877098354786e-05,
      "loss": 0.9051,
      "step": 65
    },
    {
      "epoch": 2.23728813559322,
      "grad_norm": 1.2558255195617676,
      "learning_rate": 9.512278472939627e-05,
      "loss": 0.8728,
      "step": 66
    },
    {
      "epoch": 2.2711864406779663,
      "grad_norm": 1.5434508323669434,
      "learning_rate": 9.48602610101771e-05,
      "loss": 1.0494,
      "step": 67
    },
    {
      "epoch": 2.305084745762712,
      "grad_norm": 1.4870367050170898,
      "learning_rate": 9.459123786076912e-05,
      "loss": 1.1058,
      "step": 68
    },
    {
      "epoch": 2.3389830508474576,
      "grad_norm": 1.734200358390808,
      "learning_rate": 9.431575425769938e-05,
      "loss": 1.0837,
      "step": 69
    },
    {
      "epoch": 2.3728813559322033,
      "grad_norm": 1.4065618515014648,
      "learning_rate": 9.403385011349639e-05,
      "loss": 1.1508,
      "step": 70
    },
    {
      "epoch": 2.406779661016949,
      "grad_norm": 1.1244350671768188,
      "learning_rate": 9.374556627090749e-05,
      "loss": 1.1256,
      "step": 71
    },
    {
      "epoch": 2.440677966101695,
      "grad_norm": 1.910600185394287,
      "learning_rate": 9.345094449698143e-05,
      "loss": 1.1201,
      "step": 72
    },
    {
      "epoch": 2.4745762711864407,
      "grad_norm": 1.38595712184906,
      "learning_rate": 9.315002747701715e-05,
      "loss": 0.9461,
      "step": 73
    },
    {
      "epoch": 2.5084745762711864,
      "grad_norm": 1.5522170066833496,
      "learning_rate": 9.284285880837946e-05,
      "loss": 1.2511,
      "step": 74
    },
    {
      "epoch": 2.542372881355932,
      "grad_norm": 1.3251912593841553,
      "learning_rate": 9.252948299418254e-05,
      "loss": 1.2641,
      "step": 75
    },
    {
      "epoch": 2.576271186440678,
      "grad_norm": 1.4916528463363647,
      "learning_rate": 9.220994543684224e-05,
      "loss": 0.9594,
      "step": 76
    },
    {
      "epoch": 2.610169491525424,
      "grad_norm": 1.349212408065796,
      "learning_rate": 9.188429243149824e-05,
      "loss": 0.8204,
      "step": 77
    },
    {
      "epoch": 2.6440677966101696,
      "grad_norm": 1.151559591293335,
      "learning_rate": 9.155257115930652e-05,
      "loss": 1.113,
      "step": 78
    },
    {
      "epoch": 2.6779661016949152,
      "grad_norm": 1.521554946899414,
      "learning_rate": 9.121482968060384e-05,
      "loss": 1.0495,
      "step": 79
    },
    {
      "epoch": 2.711864406779661,
      "grad_norm": 1.777960181236267,
      "learning_rate": 9.087111692794459e-05,
      "loss": 0.9774,
      "step": 80
    },
    {
      "epoch": 2.7457627118644066,
      "grad_norm": 1.6524956226348877,
      "learning_rate": 9.052148269901145e-05,
      "loss": 1.2322,
      "step": 81
    },
    {
      "epoch": 2.7796610169491527,
      "grad_norm": 1.3348642587661743,
      "learning_rate": 9.01659776494005e-05,
      "loss": 0.9522,
      "step": 82
    },
    {
      "epoch": 2.8135593220338984,
      "grad_norm": 1.6467071771621704,
      "learning_rate": 8.980465328528219e-05,
      "loss": 1.0478,
      "step": 83
    },
    {
      "epoch": 2.847457627118644,
      "grad_norm": 1.284143090248108,
      "learning_rate": 8.943756195593916e-05,
      "loss": 1.0096,
      "step": 84
    },
    {
      "epoch": 2.8813559322033897,
      "grad_norm": 1.799763560295105,
      "learning_rate": 8.906475684618158e-05,
      "loss": 1.061,
      "step": 85
    },
    {
      "epoch": 2.915254237288136,
      "grad_norm": 1.3461174964904785,
      "learning_rate": 8.868629196864182e-05,
      "loss": 0.9902,
      "step": 86
    },
    {
      "epoch": 2.9491525423728815,
      "grad_norm": 1.5780616998672485,
      "learning_rate": 8.83022221559489e-05,
      "loss": 0.9365,
      "step": 87
    },
    {
      "epoch": 2.983050847457627,
      "grad_norm": 1.5738563537597656,
      "learning_rate": 8.791260305278433e-05,
      "loss": 0.9671,
      "step": 88
    },
    {
      "epoch": 3.016949152542373,
      "grad_norm": 1.6347110271453857,
      "learning_rate": 8.751749110782012e-05,
      "loss": 0.8627,
      "step": 89
    },
    {
      "epoch": 3.0508474576271185,
      "grad_norm": 1.68912935256958,
      "learning_rate": 8.71169435655405e-05,
      "loss": 0.7554,
      "step": 90
    },
    {
      "epoch": 3.084745762711864,
      "grad_norm": 2.02402400970459,
      "learning_rate": 8.671101845794816e-05,
      "loss": 1.0578,
      "step": 91
    },
    {
      "epoch": 3.1186440677966103,
      "grad_norm": 1.8462740182876587,
      "learning_rate": 8.629977459615655e-05,
      "loss": 0.949,
      "step": 92
    },
    {
      "epoch": 3.152542372881356,
      "grad_norm": 1.8803828954696655,
      "learning_rate": 8.588327156186915e-05,
      "loss": 0.9368,
      "step": 93
    },
    {
      "epoch": 3.1864406779661016,
      "grad_norm": 1.4323526620864868,
      "learning_rate": 8.546156969874723e-05,
      "loss": 1.073,
      "step": 94
    },
    {
      "epoch": 3.2203389830508473,
      "grad_norm": 1.7111629247665405,
      "learning_rate": 8.503473010366713e-05,
      "loss": 0.8978,
      "step": 95
    },
    {
      "epoch": 3.2542372881355934,
      "grad_norm": 1.6402634382247925,
      "learning_rate": 8.460281461786847e-05,
      "loss": 0.9778,
      "step": 96
    },
    {
      "epoch": 3.288135593220339,
      "grad_norm": 1.5887268781661987,
      "learning_rate": 8.416588581799447e-05,
      "loss": 0.9765,
      "step": 97
    },
    {
      "epoch": 3.3220338983050848,
      "grad_norm": 1.6225745677947998,
      "learning_rate": 8.37240070070257e-05,
      "loss": 0.9531,
      "step": 98
    },
    {
      "epoch": 3.3559322033898304,
      "grad_norm": 1.46282160282135,
      "learning_rate": 8.327724220510873e-05,
      "loss": 0.8988,
      "step": 99
    },
    {
      "epoch": 3.389830508474576,
      "grad_norm": 1.704675316810608,
      "learning_rate": 8.282565614028067e-05,
      "loss": 0.9717,
      "step": 100
    },
    {
      "epoch": 3.423728813559322,
      "grad_norm": 1.603314995765686,
      "learning_rate": 8.236931423909138e-05,
      "loss": 0.9772,
      "step": 101
    },
    {
      "epoch": 3.457627118644068,
      "grad_norm": 2.004333734512329,
      "learning_rate": 8.190828261712429e-05,
      "loss": 0.9751,
      "step": 102
    },
    {
      "epoch": 3.4915254237288136,
      "grad_norm": 1.4294606447219849,
      "learning_rate": 8.144262806941742e-05,
      "loss": 0.9553,
      "step": 103
    },
    {
      "epoch": 3.5254237288135593,
      "grad_norm": 2.1355888843536377,
      "learning_rate": 8.097241806078615e-05,
      "loss": 0.9343,
      "step": 104
    },
    {
      "epoch": 3.559322033898305,
      "grad_norm": 1.487553596496582,
      "learning_rate": 8.049772071604865e-05,
      "loss": 0.9739,
      "step": 105
    },
    {
      "epoch": 3.593220338983051,
      "grad_norm": 1.48127019405365,
      "learning_rate": 8.001860481015593e-05,
      "loss": 0.9127,
      "step": 106
    },
    {
      "epoch": 3.6271186440677967,
      "grad_norm": 1.643784999847412,
      "learning_rate": 7.953513975822755e-05,
      "loss": 1.0779,
      "step": 107
    },
    {
      "epoch": 3.6610169491525424,
      "grad_norm": 1.820120096206665,
      "learning_rate": 7.904739560549475e-05,
      "loss": 0.943,
      "step": 108
    },
    {
      "epoch": 3.694915254237288,
      "grad_norm": 1.5412342548370361,
      "learning_rate": 7.855544301715203e-05,
      "loss": 0.839,
      "step": 109
    },
    {
      "epoch": 3.7288135593220337,
      "grad_norm": 1.9082005023956299,
      "learning_rate": 7.805935326811912e-05,
      "loss": 0.9219,
      "step": 110
    },
    {
      "epoch": 3.7627118644067794,
      "grad_norm": 1.438043475151062,
      "learning_rate": 7.755919823271465e-05,
      "loss": 0.8723,
      "step": 111
    },
    {
      "epoch": 3.7966101694915255,
      "grad_norm": 1.3207249641418457,
      "learning_rate": 7.70550503742427e-05,
      "loss": 1.0169,
      "step": 112
    },
    {
      "epoch": 3.830508474576271,
      "grad_norm": 1.8060184717178345,
      "learning_rate": 7.654698273449435e-05,
      "loss": 0.9626,
      "step": 113
    },
    {
      "epoch": 3.864406779661017,
      "grad_norm": 1.330644965171814,
      "learning_rate": 7.603506892316512e-05,
      "loss": 1.0226,
      "step": 114
    },
    {
      "epoch": 3.898305084745763,
      "grad_norm": 1.7491235733032227,
      "learning_rate": 7.551938310719043e-05,
      "loss": 1.1213,
      "step": 115
    },
    {
      "epoch": 3.9322033898305087,
      "grad_norm": 1.733307123184204,
      "learning_rate": 7.500000000000001e-05,
      "loss": 0.7787,
      "step": 116
    },
    {
      "epoch": 3.9661016949152543,
      "grad_norm": 2.0604991912841797,
      "learning_rate": 7.447699485069341e-05,
      "loss": 0.7815,
      "step": 117
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.7430380582809448,
      "learning_rate": 7.395044343313778e-05,
      "loss": 0.7575,
      "step": 118
    },
    {
      "epoch": 4.033898305084746,
      "grad_norm": 1.6886321306228638,
      "learning_rate": 7.342042203498951e-05,
      "loss": 0.8228,
      "step": 119
    },
    {
      "epoch": 4.067796610169491,
      "grad_norm": 1.7939422130584717,
      "learning_rate": 7.288700744664167e-05,
      "loss": 0.6882,
      "step": 120
    },
    {
      "epoch": 4.101694915254237,
      "grad_norm": 1.8237851858139038,
      "learning_rate": 7.235027695009846e-05,
      "loss": 0.9558,
      "step": 121
    },
    {
      "epoch": 4.135593220338983,
      "grad_norm": 1.6047351360321045,
      "learning_rate": 7.181030830777837e-05,
      "loss": 0.8948,
      "step": 122
    },
    {
      "epoch": 4.169491525423728,
      "grad_norm": 1.6723320484161377,
      "learning_rate": 7.1267179751248e-05,
      "loss": 0.995,
      "step": 123
    },
    {
      "epoch": 4.203389830508475,
      "grad_norm": 1.459901213645935,
      "learning_rate": 7.07209699698876e-05,
      "loss": 0.7848,
      "step": 124
    },
    {
      "epoch": 4.237288135593221,
      "grad_norm": 1.8700064420700073,
      "learning_rate": 7.017175809949044e-05,
      "loss": 1.0482,
      "step": 125
    },
    {
      "epoch": 4.271186440677966,
      "grad_norm": 2.3090994358062744,
      "learning_rate": 6.961962371079753e-05,
      "loss": 0.671,
      "step": 126
    },
    {
      "epoch": 4.305084745762712,
      "grad_norm": 1.8771541118621826,
      "learning_rate": 6.906464679796927e-05,
      "loss": 0.7694,
      "step": 127
    },
    {
      "epoch": 4.338983050847458,
      "grad_norm": 1.829629898071289,
      "learning_rate": 6.850690776699573e-05,
      "loss": 0.9454,
      "step": 128
    },
    {
      "epoch": 4.372881355932203,
      "grad_norm": 1.7409473657608032,
      "learning_rate": 6.79464874240473e-05,
      "loss": 0.9624,
      "step": 129
    },
    {
      "epoch": 4.406779661016949,
      "grad_norm": 2.1930043697357178,
      "learning_rate": 6.738346696376738e-05,
      "loss": 0.8286,
      "step": 130
    },
    {
      "epoch": 4.440677966101695,
      "grad_norm": 1.8771957159042358,
      "learning_rate": 6.681792795750875e-05,
      "loss": 0.7697,
      "step": 131
    },
    {
      "epoch": 4.47457627118644,
      "grad_norm": 2.345898389816284,
      "learning_rate": 6.624995234151539e-05,
      "loss": 0.7687,
      "step": 132
    },
    {
      "epoch": 4.508474576271187,
      "grad_norm": 1.5804108381271362,
      "learning_rate": 6.567962240505136e-05,
      "loss": 0.7926,
      "step": 133
    },
    {
      "epoch": 4.5423728813559325,
      "grad_norm": 2.21649432182312,
      "learning_rate": 6.510702077847863e-05,
      "loss": 0.9041,
      "step": 134
    },
    {
      "epoch": 4.576271186440678,
      "grad_norm": 1.8834631443023682,
      "learning_rate": 6.453223042128555e-05,
      "loss": 0.8793,
      "step": 135
    },
    {
      "epoch": 4.610169491525424,
      "grad_norm": 1.5182900428771973,
      "learning_rate": 6.395533461006735e-05,
      "loss": 0.9688,
      "step": 136
    },
    {
      "epoch": 4.6440677966101696,
      "grad_norm": 1.9162862300872803,
      "learning_rate": 6.337641692646106e-05,
      "loss": 0.8839,
      "step": 137
    },
    {
      "epoch": 4.677966101694915,
      "grad_norm": 2.324068307876587,
      "learning_rate": 6.279556124503589e-05,
      "loss": 0.7281,
      "step": 138
    },
    {
      "epoch": 4.711864406779661,
      "grad_norm": 1.749930739402771,
      "learning_rate": 6.221285172114157e-05,
      "loss": 0.7977,
      "step": 139
    },
    {
      "epoch": 4.745762711864407,
      "grad_norm": 1.9627902507781982,
      "learning_rate": 6.162837277871553e-05,
      "loss": 0.8023,
      "step": 140
    },
    {
      "epoch": 4.779661016949152,
      "grad_norm": 1.558073878288269,
      "learning_rate": 6.104220909805162e-05,
      "loss": 1.0994,
      "step": 141
    },
    {
      "epoch": 4.813559322033898,
      "grad_norm": 2.586397409439087,
      "learning_rate": 6.045444560353136e-05,
      "loss": 0.9047,
      "step": 142
    },
    {
      "epoch": 4.847457627118644,
      "grad_norm": 1.9306085109710693,
      "learning_rate": 5.9865167451320005e-05,
      "loss": 0.9874,
      "step": 143
    },
    {
      "epoch": 4.88135593220339,
      "grad_norm": 1.9450160264968872,
      "learning_rate": 5.927446001702899e-05,
      "loss": 0.6685,
      "step": 144
    },
    {
      "epoch": 4.915254237288136,
      "grad_norm": 2.1505260467529297,
      "learning_rate": 5.868240888334653e-05,
      "loss": 0.7977,
      "step": 145
    },
    {
      "epoch": 4.9491525423728815,
      "grad_norm": 1.6799077987670898,
      "learning_rate": 5.808909982763825e-05,
      "loss": 0.7867,
      "step": 146
    },
    {
      "epoch": 4.983050847457627,
      "grad_norm": 1.5510778427124023,
      "learning_rate": 5.749461880951966e-05,
      "loss": 0.7192,
      "step": 147
    },
    {
      "epoch": 5.016949152542373,
      "grad_norm": 1.6423349380493164,
      "learning_rate": 5.689905195840216e-05,
      "loss": 0.8374,
      "step": 148
    },
    {
      "epoch": 5.0508474576271185,
      "grad_norm": 1.4661235809326172,
      "learning_rate": 5.6302485561014475e-05,
      "loss": 0.9012,
      "step": 149
    },
    {
      "epoch": 5.084745762711864,
      "grad_norm": 1.6652096509933472,
      "learning_rate": 5.5705006048901244e-05,
      "loss": 0.9667,
      "step": 150
    },
    {
      "epoch": 5.11864406779661,
      "grad_norm": 1.8747127056121826,
      "learning_rate": 5.5106699985900736e-05,
      "loss": 0.6984,
      "step": 151
    },
    {
      "epoch": 5.1525423728813555,
      "grad_norm": 2.218189001083374,
      "learning_rate": 5.4507654055603275e-05,
      "loss": 0.6574,
      "step": 152
    },
    {
      "epoch": 5.186440677966102,
      "grad_norm": 1.5920519828796387,
      "learning_rate": 5.390795504879242e-05,
      "loss": 0.6268,
      "step": 153
    },
    {
      "epoch": 5.220338983050848,
      "grad_norm": 1.7973299026489258,
      "learning_rate": 5.330768985087059e-05,
      "loss": 0.8212,
      "step": 154
    },
    {
      "epoch": 5.254237288135593,
      "grad_norm": 1.8543109893798828,
      "learning_rate": 5.270694542927088e-05,
      "loss": 1.0803,
      "step": 155
    },
    {
      "epoch": 5.288135593220339,
      "grad_norm": 2.019928455352783,
      "learning_rate": 5.210580882085713e-05,
      "loss": 0.6047,
      "step": 156
    },
    {
      "epoch": 5.322033898305085,
      "grad_norm": 2.102348804473877,
      "learning_rate": 5.1504367119313865e-05,
      "loss": 0.763,
      "step": 157
    },
    {
      "epoch": 5.3559322033898304,
      "grad_norm": 1.889079213142395,
      "learning_rate": 5.090270746252802e-05,
      "loss": 0.9373,
      "step": 158
    },
    {
      "epoch": 5.389830508474576,
      "grad_norm": 2.1885173320770264,
      "learning_rate": 5.030091701996428e-05,
      "loss": 0.6711,
      "step": 159
    },
    {
      "epoch": 5.423728813559322,
      "grad_norm": 2.802832841873169,
      "learning_rate": 4.969908298003573e-05,
      "loss": 0.5946,
      "step": 160
    },
    {
      "epoch": 5.4576271186440675,
      "grad_norm": 3.092135190963745,
      "learning_rate": 4.909729253747197e-05,
      "loss": 0.7327,
      "step": 161
    },
    {
      "epoch": 5.491525423728813,
      "grad_norm": 1.6200447082519531,
      "learning_rate": 4.8495632880686154e-05,
      "loss": 0.8286,
      "step": 162
    },
    {
      "epoch": 5.52542372881356,
      "grad_norm": 1.8728711605072021,
      "learning_rate": 4.7894191179142876e-05,
      "loss": 0.9048,
      "step": 163
    },
    {
      "epoch": 5.559322033898305,
      "grad_norm": 2.616438150405884,
      "learning_rate": 4.729305457072913e-05,
      "loss": 0.6911,
      "step": 164
    },
    {
      "epoch": 5.593220338983051,
      "grad_norm": 1.5780282020568848,
      "learning_rate": 4.669231014912943e-05,
      "loss": 0.5329,
      "step": 165
    },
    {
      "epoch": 5.627118644067797,
      "grad_norm": 1.6153064966201782,
      "learning_rate": 4.6092044951207583e-05,
      "loss": 0.9232,
      "step": 166
    },
    {
      "epoch": 5.661016949152542,
      "grad_norm": 2.6229894161224365,
      "learning_rate": 4.549234594439674e-05,
      "loss": 0.7286,
      "step": 167
    },
    {
      "epoch": 5.694915254237288,
      "grad_norm": 1.969491720199585,
      "learning_rate": 4.489330001409928e-05,
      "loss": 0.8045,
      "step": 168
    },
    {
      "epoch": 5.728813559322034,
      "grad_norm": 2.0788402557373047,
      "learning_rate": 4.429499395109877e-05,
      "loss": 0.6663,
      "step": 169
    },
    {
      "epoch": 5.762711864406779,
      "grad_norm": 1.7995866537094116,
      "learning_rate": 4.3697514438985536e-05,
      "loss": 0.7905,
      "step": 170
    },
    {
      "epoch": 5.796610169491525,
      "grad_norm": 2.0666608810424805,
      "learning_rate": 4.310094804159784e-05,
      "loss": 0.8535,
      "step": 171
    },
    {
      "epoch": 5.830508474576272,
      "grad_norm": 1.591664433479309,
      "learning_rate": 4.250538119048036e-05,
      "loss": 0.9245,
      "step": 172
    },
    {
      "epoch": 5.864406779661017,
      "grad_norm": 1.5960257053375244,
      "learning_rate": 4.1910900172361764e-05,
      "loss": 1.006,
      "step": 173
    },
    {
      "epoch": 5.898305084745763,
      "grad_norm": 1.4143214225769043,
      "learning_rate": 4.131759111665349e-05,
      "loss": 0.8137,
      "step": 174
    },
    {
      "epoch": 5.932203389830509,
      "grad_norm": 1.673211932182312,
      "learning_rate": 4.072553998297103e-05,
      "loss": 0.8174,
      "step": 175
    },
    {
      "epoch": 5.966101694915254,
      "grad_norm": 2.1118900775909424,
      "learning_rate": 4.0134832548680006e-05,
      "loss": 0.6581,
      "step": 176
    },
    {
      "epoch": 6.0,
      "grad_norm": 2.7098617553710938,
      "learning_rate": 3.9545554396468655e-05,
      "loss": 0.592,
      "step": 177
    },
    {
      "epoch": 6.033898305084746,
      "grad_norm": 1.8049975633621216,
      "learning_rate": 3.895779090194839e-05,
      "loss": 0.689,
      "step": 178
    },
    {
      "epoch": 6.067796610169491,
      "grad_norm": 2.0893802642822266,
      "learning_rate": 3.8371627221284495e-05,
      "loss": 0.7411,
      "step": 179
    },
    {
      "epoch": 6.101694915254237,
      "grad_norm": 1.8889238834381104,
      "learning_rate": 3.778714827885845e-05,
      "loss": 0.8245,
      "step": 180
    },
    {
      "epoch": 6.135593220338983,
      "grad_norm": 1.577343463897705,
      "learning_rate": 3.720443875496411e-05,
      "loss": 0.9617,
      "step": 181
    },
    {
      "epoch": 6.169491525423728,
      "grad_norm": 1.7403689622879028,
      "learning_rate": 3.6623583073538966e-05,
      "loss": 0.6761,
      "step": 182
    },
    {
      "epoch": 6.203389830508475,
      "grad_norm": 1.6684532165527344,
      "learning_rate": 3.604466538993266e-05,
      "loss": 0.6078,
      "step": 183
    },
    {
      "epoch": 6.237288135593221,
      "grad_norm": 1.6129385232925415,
      "learning_rate": 3.546776957871445e-05,
      "loss": 0.6837,
      "step": 184
    },
    {
      "epoch": 6.271186440677966,
      "grad_norm": 1.8662892580032349,
      "learning_rate": 3.489297922152136e-05,
      "loss": 0.7305,
      "step": 185
    },
    {
      "epoch": 6.305084745762712,
      "grad_norm": 1.7838459014892578,
      "learning_rate": 3.4320377594948666e-05,
      "loss": 0.8482,
      "step": 186
    },
    {
      "epoch": 6.338983050847458,
      "grad_norm": 2.5748822689056396,
      "learning_rate": 3.375004765848463e-05,
      "loss": 0.5105,
      "step": 187
    },
    {
      "epoch": 6.372881355932203,
      "grad_norm": 2.032487630844116,
      "learning_rate": 3.3182072042491244e-05,
      "loss": 0.71,
      "step": 188
    },
    {
      "epoch": 6.406779661016949,
      "grad_norm": 2.3323135375976562,
      "learning_rate": 3.261653303623263e-05,
      "loss": 0.6663,
      "step": 189
    },
    {
      "epoch": 6.440677966101695,
      "grad_norm": 1.9065417051315308,
      "learning_rate": 3.205351257595272e-05,
      "loss": 0.6512,
      "step": 190
    },
    {
      "epoch": 6.47457627118644,
      "grad_norm": 1.846157193183899,
      "learning_rate": 3.149309223300428e-05,
      "loss": 0.6225,
      "step": 191
    },
    {
      "epoch": 6.508474576271187,
      "grad_norm": 1.8569318056106567,
      "learning_rate": 3.0935353202030734e-05,
      "loss": 0.8757,
      "step": 192
    },
    {
      "epoch": 6.5423728813559325,
      "grad_norm": 1.8902487754821777,
      "learning_rate": 3.0380376289202495e-05,
      "loss": 0.6608,
      "step": 193
    },
    {
      "epoch": 6.576271186440678,
      "grad_norm": 1.898270606994629,
      "learning_rate": 2.982824190050958e-05,
      "loss": 0.8399,
      "step": 194
    },
    {
      "epoch": 6.610169491525424,
      "grad_norm": 2.0619301795959473,
      "learning_rate": 2.9279030030112408e-05,
      "loss": 0.9557,
      "step": 195
    },
    {
      "epoch": 6.6440677966101696,
      "grad_norm": 2.059544324874878,
      "learning_rate": 2.8732820248752013e-05,
      "loss": 0.3855,
      "step": 196
    },
    {
      "epoch": 6.677966101694915,
      "grad_norm": 2.0559706687927246,
      "learning_rate": 2.8189691692221627e-05,
      "loss": 0.8543,
      "step": 197
    },
    {
      "epoch": 6.711864406779661,
      "grad_norm": 2.222141981124878,
      "learning_rate": 2.7649723049901554e-05,
      "loss": 0.6373,
      "step": 198
    },
    {
      "epoch": 6.745762711864407,
      "grad_norm": 1.8821125030517578,
      "learning_rate": 2.711299255335833e-05,
      "loss": 0.6302,
      "step": 199
    },
    {
      "epoch": 6.779661016949152,
      "grad_norm": 2.1055614948272705,
      "learning_rate": 2.65795779650105e-05,
      "loss": 0.6426,
      "step": 200
    },
    {
      "epoch": 6.813559322033898,
      "grad_norm": 1.6734181642532349,
      "learning_rate": 2.6049556566862233e-05,
      "loss": 0.7696,
      "step": 201
    },
    {
      "epoch": 6.847457627118644,
      "grad_norm": 1.846907138824463,
      "learning_rate": 2.5523005149306572e-05,
      "loss": 0.7069,
      "step": 202
    },
    {
      "epoch": 6.88135593220339,
      "grad_norm": 2.158766269683838,
      "learning_rate": 2.500000000000001e-05,
      "loss": 0.637,
      "step": 203
    },
    {
      "epoch": 6.915254237288136,
      "grad_norm": 2.3312888145446777,
      "learning_rate": 2.4480616892809594e-05,
      "loss": 0.7527,
      "step": 204
    },
    {
      "epoch": 6.9491525423728815,
      "grad_norm": 2.0084948539733887,
      "learning_rate": 2.396493107683488e-05,
      "loss": 0.8419,
      "step": 205
    },
    {
      "epoch": 6.983050847457627,
      "grad_norm": 2.4590446949005127,
      "learning_rate": 2.3453017265505673e-05,
      "loss": 0.8292,
      "step": 206
    },
    {
      "epoch": 7.016949152542373,
      "grad_norm": 1.6941362619400024,
      "learning_rate": 2.2944949625757294e-05,
      "loss": 0.7468,
      "step": 207
    },
    {
      "epoch": 7.0508474576271185,
      "grad_norm": 1.8564373254776,
      "learning_rate": 2.2440801767285357e-05,
      "loss": 0.7359,
      "step": 208
    },
    {
      "epoch": 7.084745762711864,
      "grad_norm": 1.5862066745758057,
      "learning_rate": 2.194064673188089e-05,
      "loss": 0.7298,
      "step": 209
    },
    {
      "epoch": 7.11864406779661,
      "grad_norm": 1.70759117603302,
      "learning_rate": 2.1444556982847997e-05,
      "loss": 0.8768,
      "step": 210
    },
    {
      "epoch": 7.1525423728813555,
      "grad_norm": 2.2542805671691895,
      "learning_rate": 2.095260439450526e-05,
      "loss": 0.5523,
      "step": 211
    },
    {
      "epoch": 7.186440677966102,
      "grad_norm": 2.0315158367156982,
      "learning_rate": 2.0464860241772455e-05,
      "loss": 0.6909,
      "step": 212
    },
    {
      "epoch": 7.220338983050848,
      "grad_norm": 1.875026822090149,
      "learning_rate": 1.9981395189844086e-05,
      "loss": 0.6356,
      "step": 213
    },
    {
      "epoch": 7.254237288135593,
      "grad_norm": 1.6218189001083374,
      "learning_rate": 1.9502279283951364e-05,
      "loss": 0.4767,
      "step": 214
    },
    {
      "epoch": 7.288135593220339,
      "grad_norm": 1.64661705493927,
      "learning_rate": 1.902758193921385e-05,
      "loss": 0.712,
      "step": 215
    },
    {
      "epoch": 7.322033898305085,
      "grad_norm": 2.2401123046875,
      "learning_rate": 1.855737193058258e-05,
      "loss": 0.6705,
      "step": 216
    },
    {
      "epoch": 7.3559322033898304,
      "grad_norm": 1.8117015361785889,
      "learning_rate": 1.8091717382875722e-05,
      "loss": 0.6596,
      "step": 217
    },
    {
      "epoch": 7.389830508474576,
      "grad_norm": 1.9022445678710938,
      "learning_rate": 1.7630685760908622e-05,
      "loss": 0.6669,
      "step": 218
    },
    {
      "epoch": 7.423728813559322,
      "grad_norm": 1.936079740524292,
      "learning_rate": 1.7174343859719333e-05,
      "loss": 0.4902,
      "step": 219
    },
    {
      "epoch": 7.4576271186440675,
      "grad_norm": 1.9871655702590942,
      "learning_rate": 1.6722757794891287e-05,
      "loss": 0.5199,
      "step": 220
    },
    {
      "epoch": 7.491525423728813,
      "grad_norm": 2.117741584777832,
      "learning_rate": 1.6275992992974308e-05,
      "loss": 0.6435,
      "step": 221
    },
    {
      "epoch": 7.52542372881356,
      "grad_norm": 2.4974217414855957,
      "learning_rate": 1.5834114182005544e-05,
      "loss": 0.8364,
      "step": 222
    },
    {
      "epoch": 7.559322033898305,
      "grad_norm": 1.781215786933899,
      "learning_rate": 1.5397185382131522e-05,
      "loss": 0.6047,
      "step": 223
    },
    {
      "epoch": 7.593220338983051,
      "grad_norm": 1.5463509559631348,
      "learning_rate": 1.4965269896332885e-05,
      "loss": 0.6514,
      "step": 224
    },
    {
      "epoch": 7.627118644067797,
      "grad_norm": 1.8010554313659668,
      "learning_rate": 1.4538430301252782e-05,
      "loss": 0.8118,
      "step": 225
    },
    {
      "epoch": 7.661016949152542,
      "grad_norm": 1.6577845811843872,
      "learning_rate": 1.4116728438130861e-05,
      "loss": 0.7331,
      "step": 226
    },
    {
      "epoch": 7.694915254237288,
      "grad_norm": 1.9587113857269287,
      "learning_rate": 1.3700225403843469e-05,
      "loss": 0.9704,
      "step": 227
    },
    {
      "epoch": 7.728813559322034,
      "grad_norm": 2.0030181407928467,
      "learning_rate": 1.3288981542051842e-05,
      "loss": 0.6118,
      "step": 228
    },
    {
      "epoch": 7.762711864406779,
      "grad_norm": 2.4136457443237305,
      "learning_rate": 1.2883056434459506e-05,
      "loss": 0.7513,
      "step": 229
    },
    {
      "epoch": 7.796610169491525,
      "grad_norm": 1.8475357294082642,
      "learning_rate": 1.2482508892179884e-05,
      "loss": 0.7482,
      "step": 230
    },
    {
      "epoch": 7.830508474576272,
      "grad_norm": 2.2184154987335205,
      "learning_rate": 1.2087396947215679e-05,
      "loss": 0.6869,
      "step": 231
    },
    {
      "epoch": 7.864406779661017,
      "grad_norm": 2.1365082263946533,
      "learning_rate": 1.1697777844051105e-05,
      "loss": 0.603,
      "step": 232
    },
    {
      "epoch": 7.898305084745763,
      "grad_norm": 1.8059977293014526,
      "learning_rate": 1.1313708031358183e-05,
      "loss": 0.8346,
      "step": 233
    },
    {
      "epoch": 7.932203389830509,
      "grad_norm": 2.3462767601013184,
      "learning_rate": 1.0935243153818436e-05,
      "loss": 0.6615,
      "step": 234
    },
    {
      "epoch": 7.966101694915254,
      "grad_norm": 1.9406731128692627,
      "learning_rate": 1.0562438044060846e-05,
      "loss": 0.7652,
      "step": 235
    },
    {
      "epoch": 8.0,
      "grad_norm": 2.0434494018554688,
      "learning_rate": 1.0195346714717813e-05,
      "loss": 0.7592,
      "step": 236
    },
    {
      "epoch": 8.033898305084746,
      "grad_norm": 1.7715201377868652,
      "learning_rate": 9.834022350599537e-06,
      "loss": 0.7135,
      "step": 237
    },
    {
      "epoch": 8.067796610169491,
      "grad_norm": 1.656030297279358,
      "learning_rate": 9.478517300988559e-06,
      "loss": 0.6163,
      "step": 238
    },
    {
      "epoch": 8.101694915254237,
      "grad_norm": 1.7150622606277466,
      "learning_rate": 9.12888307205541e-06,
      "loss": 0.8969,
      "step": 239
    },
    {
      "epoch": 8.135593220338983,
      "grad_norm": 1.891258716583252,
      "learning_rate": 8.785170319396175e-06,
      "loss": 0.596,
      "step": 240
    },
    {
      "epoch": 8.169491525423728,
      "grad_norm": 1.8495538234710693,
      "learning_rate": 8.447428840693488e-06,
      "loss": 0.7959,
      "step": 241
    },
    {
      "epoch": 8.203389830508474,
      "grad_norm": 1.7592411041259766,
      "learning_rate": 8.115707568501768e-06,
      "loss": 0.6817,
      "step": 242
    },
    {
      "epoch": 8.23728813559322,
      "grad_norm": 1.5580006837844849,
      "learning_rate": 7.790054563157745e-06,
      "loss": 0.5535,
      "step": 243
    },
    {
      "epoch": 8.271186440677965,
      "grad_norm": 1.8951750993728638,
      "learning_rate": 7.470517005817474e-06,
      "loss": 0.7376,
      "step": 244
    },
    {
      "epoch": 8.305084745762711,
      "grad_norm": 2.0628082752227783,
      "learning_rate": 7.157141191620548e-06,
      "loss": 0.7041,
      "step": 245
    },
    {
      "epoch": 8.338983050847457,
      "grad_norm": 1.5957227945327759,
      "learning_rate": 6.849972522982845e-06,
      "loss": 0.6579,
      "step": 246
    },
    {
      "epoch": 8.372881355932204,
      "grad_norm": 2.149848461151123,
      "learning_rate": 6.549055503018575e-06,
      "loss": 0.5516,
      "step": 247
    },
    {
      "epoch": 8.40677966101695,
      "grad_norm": 1.6677641868591309,
      "learning_rate": 6.2544337290925185e-06,
      "loss": 0.4954,
      "step": 248
    },
    {
      "epoch": 8.440677966101696,
      "grad_norm": 1.762616515159607,
      "learning_rate": 5.966149886503614e-06,
      "loss": 0.6981,
      "step": 249
    },
    {
      "epoch": 8.474576271186441,
      "grad_norm": 1.9421274662017822,
      "learning_rate": 5.684245742300625e-06,
      "loss": 0.449,
      "step": 250
    },
    {
      "epoch": 8.508474576271187,
      "grad_norm": 1.9067823886871338,
      "learning_rate": 5.408762139230888e-06,
      "loss": 0.6712,
      "step": 251
    },
    {
      "epoch": 8.542372881355933,
      "grad_norm": 1.6977453231811523,
      "learning_rate": 5.139738989822901e-06,
      "loss": 0.7866,
      "step": 252
    },
    {
      "epoch": 8.576271186440678,
      "grad_norm": 1.7816241979599,
      "learning_rate": 4.877215270603753e-06,
      "loss": 0.8108,
      "step": 253
    },
    {
      "epoch": 8.610169491525424,
      "grad_norm": 1.9886852502822876,
      "learning_rate": 4.621229016452156e-06,
      "loss": 0.6678,
      "step": 254
    },
    {
      "epoch": 8.64406779661017,
      "grad_norm": 1.9169764518737793,
      "learning_rate": 4.371817315087845e-06,
      "loss": 0.7357,
      "step": 255
    },
    {
      "epoch": 8.677966101694915,
      "grad_norm": 2.145796060562134,
      "learning_rate": 4.129016301698285e-06,
      "loss": 0.4335,
      "step": 256
    },
    {
      "epoch": 8.711864406779661,
      "grad_norm": 2.450910806655884,
      "learning_rate": 3.892861153703342e-06,
      "loss": 0.7454,
      "step": 257
    },
    {
      "epoch": 8.745762711864407,
      "grad_norm": 1.8134201765060425,
      "learning_rate": 3.6633860856586932e-06,
      "loss": 0.895,
      "step": 258
    },
    {
      "epoch": 8.779661016949152,
      "grad_norm": 1.736405611038208,
      "learning_rate": 3.4406243442987764e-06,
      "loss": 0.6404,
      "step": 259
    },
    {
      "epoch": 8.813559322033898,
      "grad_norm": 2.1191558837890625,
      "learning_rate": 3.2246082037199532e-06,
      "loss": 0.7308,
      "step": 260
    },
    {
      "epoch": 8.847457627118644,
      "grad_norm": 2.0437142848968506,
      "learning_rate": 3.0153689607045845e-06,
      "loss": 0.5955,
      "step": 261
    },
    {
      "epoch": 8.88135593220339,
      "grad_norm": 1.8263020515441895,
      "learning_rate": 2.8129369301866883e-06,
      "loss": 0.755,
      "step": 262
    },
    {
      "epoch": 8.915254237288135,
      "grad_norm": 1.6495636701583862,
      "learning_rate": 2.6173414408598827e-06,
      "loss": 0.46,
      "step": 263
    },
    {
      "epoch": 8.94915254237288,
      "grad_norm": 1.6345099210739136,
      "learning_rate": 2.428610830928152e-06,
      "loss": 0.7236,
      "step": 264
    },
    {
      "epoch": 8.983050847457626,
      "grad_norm": 1.8403527736663818,
      "learning_rate": 2.2467724440002336e-06,
      "loss": 0.6645,
      "step": 265
    },
    {
      "epoch": 9.016949152542374,
      "grad_norm": 1.888153314590454,
      "learning_rate": 2.0718526251279346e-06,
      "loss": 0.8162,
      "step": 266
    },
    {
      "epoch": 9.05084745762712,
      "grad_norm": 1.7123456001281738,
      "learning_rate": 1.9038767169893056e-06,
      "loss": 0.7258,
      "step": 267
    },
    {
      "epoch": 9.084745762711865,
      "grad_norm": 1.9329018592834473,
      "learning_rate": 1.7428690562169004e-06,
      "loss": 0.5396,
      "step": 268
    },
    {
      "epoch": 9.11864406779661,
      "grad_norm": 1.7614383697509766,
      "learning_rate": 1.5888529698718346e-06,
      "loss": 0.7247,
      "step": 269
    },
    {
      "epoch": 9.152542372881356,
      "grad_norm": 1.8571826219558716,
      "learning_rate": 1.4418507720641793e-06,
      "loss": 0.8294,
      "step": 270
    },
    {
      "epoch": 9.186440677966102,
      "grad_norm": 1.852707862854004,
      "learning_rate": 1.3018837607199906e-06,
      "loss": 0.7018,
      "step": 271
    },
    {
      "epoch": 9.220338983050848,
      "grad_norm": 1.6590862274169922,
      "learning_rate": 1.1689722144956671e-06,
      "loss": 0.7808,
      "step": 272
    },
    {
      "epoch": 9.254237288135593,
      "grad_norm": 1.8914035558700562,
      "learning_rate": 1.0431353898399388e-06,
      "loss": 0.862,
      "step": 273
    },
    {
      "epoch": 9.288135593220339,
      "grad_norm": 1.6389447450637817,
      "learning_rate": 9.243915182039431e-07,
      "loss": 0.6423,
      "step": 274
    },
    {
      "epoch": 9.322033898305085,
      "grad_norm": 1.6363378763198853,
      "learning_rate": 8.127578033998662e-07,
      "loss": 0.6838,
      "step": 275
    },
    {
      "epoch": 9.35593220338983,
      "grad_norm": 1.8926810026168823,
      "learning_rate": 7.082504191083416e-07,
      "loss": 0.6647,
      "step": 276
    },
    {
      "epoch": 9.389830508474576,
      "grad_norm": 1.4539774656295776,
      "learning_rate": 6.108845065352864e-07,
      "loss": 0.5243,
      "step": 277
    },
    {
      "epoch": 9.423728813559322,
      "grad_norm": 1.8619297742843628,
      "learning_rate": 5.206741722181386e-07,
      "loss": 0.7495,
      "step": 278
    },
    {
      "epoch": 9.457627118644067,
      "grad_norm": 1.8209561109542847,
      "learning_rate": 4.376324859820924e-07,
      "loss": 0.6018,
      "step": 279
    },
    {
      "epoch": 9.491525423728813,
      "grad_norm": 1.846883773803711,
      "learning_rate": 3.617714790465576e-07,
      "loss": 0.7567,
      "step": 280
    },
    {
      "epoch": 9.525423728813559,
      "grad_norm": 1.4403468370437622,
      "learning_rate": 2.9310214228202013e-07,
      "loss": 0.4541,
      "step": 281
    },
    {
      "epoch": 9.559322033898304,
      "grad_norm": 2.0953142642974854,
      "learning_rate": 2.3163442461766606e-07,
      "loss": 0.4852,
      "step": 282
    },
    {
      "epoch": 9.59322033898305,
      "grad_norm": 1.6899442672729492,
      "learning_rate": 1.7737723159998997e-07,
      "loss": 0.4878,
      "step": 283
    },
    {
      "epoch": 9.627118644067796,
      "grad_norm": 1.6317211389541626,
      "learning_rate": 1.3033842410251075e-07,
      "loss": 0.6025,
      "step": 284
    },
    {
      "epoch": 9.661016949152543,
      "grad_norm": 2.2324962615966797,
      "learning_rate": 9.052481718690997e-08,
      "loss": 0.5914,
      "step": 285
    },
    {
      "epoch": 9.694915254237289,
      "grad_norm": 2.174980878829956,
      "learning_rate": 5.794217911562205e-08,
      "loss": 0.5414,
      "step": 286
    },
    {
      "epoch": 9.728813559322035,
      "grad_norm": 1.9721323251724243,
      "learning_rate": 3.259523051615254e-08,
      "loss": 0.7042,
      "step": 287
    },
    {
      "epoch": 9.76271186440678,
      "grad_norm": 2.325942039489746,
      "learning_rate": 1.4487643697103092e-08,
      "loss": 0.7828,
      "step": 288
    },
    {
      "epoch": 9.796610169491526,
      "grad_norm": 1.8012490272521973,
      "learning_rate": 3.622042116169233e-09,
      "loss": 0.74,
      "step": 289
    },
    {
      "epoch": 9.830508474576272,
      "grad_norm": 2.070490598678589,
      "learning_rate": 0.0,
      "loss": 0.5188,
      "step": 290
    }
  ],
  "logging_steps": 1,
  "max_steps": 290,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4978641968228352.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
